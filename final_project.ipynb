{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marco-scatassi/CasualNetworkProject/blob/main/final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFEE6cVcamHi"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%apt install python3-dev graphviz libgraphviz-dev pkg-config\n",
        "%pip install networkx pygraphviz matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LvgkKcGJtMf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "from networkx.drawing.nx_agraph import graphviz_layout\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvKlwMvNOrw7"
      },
      "outputs": [],
      "source": [
        "# Get machine epsilon.\n",
        "e = np.finfo(np.float64).eps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFpk2Fi6bs1-"
      },
      "source": [
        "# Learning Bayesian Networks from Big Data with Greedy Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNfpPGO0boYG"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRXEwZKl2j4O"
      },
      "source": [
        "One of the main important task in the causal framework is the **causal discovery task**. The aim of which is to find the **causal model** underline some given observational data. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In particular, using the tool of **causal network**, the discovery task became the task of identifying:\n",
        "1. a Directed Acyclic Graph (DAG) and \n",
        "2. a set of conditional probability distribution.\n",
        "\n",
        "These two subtasks are referred as:\n",
        "1. structure learning\n",
        "2. parameter learning"
      ],
      "metadata": {
        "id": "FJkJI-qsATsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook the focus will be on the activity of **structure learning**. Essentially, a partial implementation of the \"**Algorithm 1** Greedy Search\" described in the paper [1] will be provided.\n"
      ],
      "metadata": {
        "id": "Teg7Xf1UAVjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Algorithm 1: Greedy Search"
      ],
      "metadata": {
        "id": "bqzfPgrVECrh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This algorithm belong to the class of the so called **score-based** algorithm. Which means that it follows a certain **methodology to explore** the space of possible DAG using a provided **score function** to evaluate candidate DAGs. "
      ],
      "metadata": {
        "id": "Juojde_GEJTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following, the portion of the Algorithm 1 that will be implemented in this notebook is described."
      ],
      "metadata": {
        "id": "guJFT3XXsRdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Alghorithm 1** Greedy Search \n",
        "\n",
        "---\n",
        "\n",
        "**Input**: a data set $D$ from $X$, an initial DAG $G$ (usually the empty DAG),\n",
        "a score function $Score(G,D)$.\n",
        "\n",
        "**Output**: the DAG $G_{max}$ that maximises $Score(G,D)$.\n",
        "1. Compute the score of $G$, $S_G = Score(G,D)$.\n",
        "2. Set $S_{max} = S_G$ and $G_{max} = G$.\n",
        "3. **Hill climbing**: repeat as long as $S_{max}$ increases:\n",
        "  1. for every possible arc addition, deletion or reversal in $G_{max}$ resulting in a DAG:\n",
        "    1. compute the score of the modified DAG $G^∗$, $S_{G^∗} = Score(G^∗,D)$:\n",
        "    2. if $S_{G^∗} > S_{max}$ and $S_{G^∗} > S_G$, set $G = G^∗$ and $S_G = S_{G^∗}$ .\n",
        "  2. if $S_G > S_{max}$, set $S_{max} = S_G$ and $G_{max} = G$.\n",
        "---"
      ],
      "metadata": {
        "id": "QePOVIWRIvCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Input"
      ],
      "metadata": {
        "id": "3_0YxiaEs3N0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The inputs needed by the algorithm are:\n",
        "1. an initial DAG $G$, that is the DAG from which the search will start\n",
        "2. a dataset $D$ and a score function $Score(G,D)$; these two elements are necessary to evaluate a DAG. Indeed, the score function provides a mathematical expression that can be used to compare a candidate DAG $G$ with collected data $D$. If the DAG has an high score, it is likely to **correctly represent** the structure of the causal model that has generated the data."
      ],
      "metadata": {
        "id": "GlM_V84JvZ3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The score function used in this notebook is the log-likelihood ($LL$) and it will be described in a specific section."
      ],
      "metadata": {
        "id": "OSDjYhW7x52c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1 / 2"
      ],
      "metadata": {
        "id": "vz1ApxfGtn_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first and second steps are simply initialization steps, and they consists in:\n",
        "1. evaluate the initial DAG $G$ \n",
        "2. initialize the value of the variables $S_{max}$ and $G_{max}$. At the end of the procedure these quantity will correspond to the optimal DAG and its value with respect to the chosen score function"
      ],
      "metadata": {
        "id": "yOQB93dW0iJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3"
      ],
      "metadata": {
        "id": "epbJm7wmttdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A brief description of **hill climbing**. The **hill climbing** is an optimization technique that follows an iteratively procedure. The method produces at each iteration a set of candidate solution modifying the previous one in some way. Then it updates the current solution only if at least one of the new candidates has a better score than the actual solution, otherwise it stops."
      ],
      "metadata": {
        "id": "0hK0Rfk82Ltk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **hill climnbing** tecnhique belongs to the so called **local search** methods. Indeed, at each iteration, it explores new possible solutions obtained using a local move starting from the actual one. \n"
      ],
      "metadata": {
        "id": "raFVxFe39vzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Furthermore, because it updates the current solutions only if a new, better solution is found, this method is only able to reach certainly the global maximum in **convex optimization** problems. Otherwise, it could get stacked in a **local maxima or minima**."
      ],
      "metadata": {
        "id": "gn7gW9WbBTqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **Algorithm 1** local moves correspond to:\n",
        "- addition\n",
        "- deletion\n",
        "- reversal\n",
        "\n",
        "the step:\n",
        "- 1.1. compute a new candidate DAG applying one of the previous moves\n",
        "- 1.2. verify if the new DAG is better that the best DAG obtained up to this moment and update the variables if necessary \n"
      ],
      "metadata": {
        "id": "V9MhQynMCw3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Output"
      ],
      "metadata": {
        "id": "3QTfXxr5tHai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DAG which maximize the score function, that is a local optimal solution or the global optimal one. "
      ],
      "metadata": {
        "id": "XuGRBk9YFXr8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5zcVqbGbpte"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UvSuE-DYKYy"
      },
      "source": [
        "### Read Data and Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mu20gfmKpNE"
      },
      "outputs": [],
      "source": [
        "# Read data from file.\n",
        "D = pd.read_csv(\"./final_project_data.csv\", dtype = \"category\")\n",
        "# Build the true graph.\n",
        "G = nx.DiGraph([\n",
        "    (\"A\", \"T\"),\n",
        "    (\"S\", \"L\"),\n",
        "    (\"S\", \"B\"),\n",
        "    (\"B\", \"D\"),\n",
        "    (\"E\", \"D\"),\n",
        "    (\"T\", \"E\"),\n",
        "    (\"L\", \"E\"),\n",
        "    (\"E\", \"X\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfEtth_saRVv"
      },
      "outputs": [],
      "source": [
        "nx.draw_networkx(G, pos = graphviz_layout(G, prog = \"dot\"), with_labels = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amsIARH0U0ON"
      },
      "source": [
        "### Computing the Absolute Frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wp7863ExJzQ-"
      },
      "outputs": [],
      "source": [
        "# Compute the absolute frequencies N(X, Pa(X) | D).\n",
        "def N(X: str, Z: List[str], D: pd.DataFrame) -> np.ndarray:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4q2TpDPZUCYj"
      },
      "outputs": [],
      "source": [
        "# Compute N(A | D).\n",
        "np.testing.assert_equal(\n",
        "    N(\"A\", [], D),\n",
        "    np.array([[4958,   42]])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sHR1CA-ULRS"
      },
      "outputs": [],
      "source": [
        "# Compute N(A, [L] | D).\n",
        "np.testing.assert_equal(\n",
        "    N(\"A\", [\"L\"], D),\n",
        "    np.array([\n",
        "        [4632,   38],\n",
        "        [ 326,    4]\n",
        "    ])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8MIKC1sK9g9"
      },
      "outputs": [],
      "source": [
        "# Compute N(A, [L, T] | D).\n",
        "np.testing.assert_equal(\n",
        "    N(\"A\", [\"L\", \"T\"], D),\n",
        "    np.array([\n",
        "        [4594,   36],\n",
        "        [  38,    2],\n",
        "        [ 322,    4],\n",
        "        [   4,    0]\n",
        "    ])\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX67s0JXUwP2"
      },
      "source": [
        "### Computing the Log-Likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmF6OYiVN1T2"
      },
      "outputs": [],
      "source": [
        "# Compute log-likelihood of LL(X, Pa(X) | D).\n",
        "def ll(X: str, Z: List[str], D: pd.DataFrame) -> float:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt0rCwR0Pvjh"
      },
      "outputs": [],
      "source": [
        "# Compute LL(A | D).\n",
        "np.testing.assert_approx_equal(\n",
        "    ll(\"A\", [], D),\n",
        "    -242.5631\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs1WetrHRB0A"
      },
      "outputs": [],
      "source": [
        "# Compute LL(A, [L] | D).\n",
        "np.testing.assert_approx_equal(\n",
        "    ll(\"A\", [\"L\"], D),\n",
        "    -242.3023\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYnoZWJjU-32"
      },
      "outputs": [],
      "source": [
        "# Compute LL(A, [L, T] | D).\n",
        "np.testing.assert_approx_equal(\n",
        "    ll(\"A\", [\"L\", \"T\"], D),\n",
        "    -240.2226\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX6wM9ClWsNJ"
      },
      "outputs": [],
      "source": [
        "# Compute log-likelihood of LL(G | D).\n",
        "def LL(G: nx.DiGraph, D: pd.DataFrame) -> float:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAdLfgKna7iC"
      },
      "outputs": [],
      "source": [
        "# Compute LL(G | D).\n",
        "np.testing.assert_approx_equal(\n",
        "    LL(G, D),\n",
        "    -11033.0871\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuvvdrESbSQk"
      },
      "source": [
        "### Implementing Hill-Climbing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUplO4qDcTFq"
      },
      "outputs": [],
      "source": [
        "# Hill-Climbing.\n",
        "def HC(D: pd.DataFrame) -> nx.DiGraph:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSmiCV42c-IQ"
      },
      "outputs": [],
      "source": [
        "H = HC(D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfqerefihDMm"
      },
      "outputs": [],
      "source": [
        "nx.draw_networkx(H, pos = graphviz_layout(H, prog = \"dot\"), with_labels = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aScEPxf62j4V"
      },
      "source": [
        "### Testing Hill-Climbing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FEOR6Sk2j4V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference"
      ],
      "metadata": {
        "id": "E9CYaejvzMfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] [Marco Scutari, Claudia Vitolo, Allan Tucker. *Learning Bayesian networks from big data with greedy search:\n",
        "computational complexity and efficient implementation*, Statistics and Computing (2019) 29:1095–1108, *https://doi.org/10.1007/s11222-019-09857-1*]"
      ],
      "metadata": {
        "id": "6WF5KQqYCJFi"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "af153e239d61b5c0d363f35fe8a0509d492506f382d017c98fa3fb49fce70a72"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}